---
title: "Details"
date :  "`r Sys.Date()`" 
weight: 3
chapter: false
pre: " <b> 9.3. </b> "
---


# ğŸš€ Deploying DeepSeek with vLLM on AWS EKS  

### ğŸ“Œ Overview  
The **DeepSeek with vLLM on EKS** project facilitates the deployment of DeepSeek models on **AWS EKS (Elastic Kubernetes Service)**, optimized for GPU or NeuronCore. Below is a step-by-step guide on setting up the infrastructure, deploying the model, and integrating the chatbot UI.  

---  
### ğŸ—ï¸ Project Structure  
```
chatbot-ui/
â”‚â”€â”€ application/
â”‚   â”œâ”€â”€ Dockerfile          # Dockerfile for building the UI container  
â”‚   â”œâ”€â”€ app.py              # Chatbot UI application (Gradio)  
â”‚   â”œâ”€â”€ requirements.txt    # Python dependencies  
â”‚
â”‚â”€â”€ manifests/
â”‚   â”œâ”€â”€ deployment.yaml     # Kubernetes Deployment for UI  
â”‚   â”œâ”€â”€ ingress-class.yaml  # Ingress configuration  
â”‚
â”‚â”€â”€ static/images/          # UI image assets  
â”‚
â”‚â”€â”€ vllm-chart/             # Helm chart for vLLM  
â”‚   â”œâ”€â”€ values.yaml         # Default configuration values  
â”‚   â”œâ”€â”€ templates/
â”‚       â”œâ”€â”€ deployment.yaml # Pod configuration  
â”‚       â”œâ”€â”€ service.yaml    # Service configuration  
â”‚       â”œâ”€â”€ _helpers.tpl    # Template helpers  
â”‚
â”‚â”€â”€ .gitignore  
â”‚â”€â”€ helm.tf                 # Terraform script for Helm configuration  
â”‚â”€â”€ main.tf                 # Terraform infrastructure setup  
â”‚â”€â”€ nodepool_automode.tf    # Auto-scaling node pool configuration  
â”‚â”€â”€ README.md               # Documentation  
```  
---  
### ğŸ”„ Workflow  

#### 1ï¸âƒ£ Infrastructure Setup with Terraform  
1. **Create VPC and EKS Cluster**  
   - Defined in `main.tf` using the `terraform-aws-modules/eks/aws` module.  
   - Configure **Auto Mode Node Pool** (`nodepool_automode.tf`) to automatically select GPU/Neuron instances.  
   - Create an Amazon ECR repository to store chatbot and vLLM images.  

2. **Configure Node Pool** (`nodepool_automode.tf`)  
   - Set up GPU (`g5/g6/p5`) or Neuron (`inf2`).  
   - Define node selectors, taints, and labels for optimized resource allocation.  

#### 2ï¸âƒ£ Deploy Model with Helm  
1. **Deploy Helm Chart (`helm.tf`)**  
   - Set up two deployments:  
     - `deepseek_gpu`: Runs on GPU.  
     - `deepseek_neuron`: Runs on Neuron.  
   - Define resource allocation, selectors, and runtime settings for vLLM.  

2. **Configure vLLM Chart (`vllm-chart/`)**  
   - `deployment.yaml`: Defines the vLLM running pod.  
   - `service.yaml`: Configures the service to handle requests to vLLM.  

#### 3ï¸âƒ£ Deploy Chatbot UI  
1. **Build UI Container (`Dockerfile`)**  
   - Base Image: `python:3.12-slim`  
   - Install dependencies from `requirements.txt`.  
   - Run the app with `uvicorn app:app --host 0.0.0.0 --port 7860`.  

2. **Deploy UI to Kubernetes (`manifests/`)**  
   - `deployment.yaml`: Creates a pod running the UI container.  
   - `ingress-class.yaml`: Configures Ingress with an **Application Load Balancer (ALB)**.  

---  
### âš™ï¸ Configuration Details  

#### ğŸŒ **AWS Infrastructure**  
- **EKS Cluster**: Deployed with Terraform.  
- **Node Pool**: Auto-selects GPU or Neuron.  
- **ECR**: Stores container images.  

#### ğŸ›  **Chatbot Application**  
- **Backend**: Uses `FastAPI` to communicate with vLLM.  
- **Frontend**: Built with `Gradio` for an interactive UI.  
- **Authentication**: Supports login via username/password.  

#### ğŸ“¦ **vLLM Deployment**  
- **GPU Model**: DeepSeek runs on GPUs (`g5/g6/p5`).  
- **Neuron Model**: DeepSeek runs on NeuronCore (`inf2`).  
- **Resource Requests & Limits**: Optimized resource allocation.  

---  
### ğŸ”’ Security  
- **IAM Role-Based Access Control** for secure permissions.  
- **SSL Certificates** with Ingress Controller.  
- **Kubernetes Secrets** for protecting credentials.  

---  
### ğŸ“Š Monitoring & Performance  
- **Liveness & Readiness Probes** for container health checks.  
- **Resource Limits** to prevent overload.  
- **Node Affinity & Taints** for efficient resource scheduling.  